{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KotkaZ/journey-to-zero/blob/master/RandomForestRegressor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "xQmC7g0vG_zk"
      },
      "source": [
        "# Random Forest Regressor Notebook\n",
        "\n",
        "This notebook contains data processing and Random Forest Regressor ensable model. The input data is a JourneyToZero Kaggle competition. \n",
        "\n",
        "https://www.kaggle.com/competitions/predict-electricity-consumption\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "z383-kW0G_zl"
      },
      "outputs": [],
      "source": [
        "# Here we use Numpy arrays and Pandas dataframes to pass and modify data.\n",
        "import numpy as nb\n",
        "import pandas as pd\n",
        "\n",
        "# We use OneHotEncoder from preprocessing sklearn module.\n",
        "from sklearn import preprocessing\n",
        "\n",
        "# We use PCA from decomposition sklearn module.\n",
        "from sklearn import decomposition\n",
        "\n",
        "# Used to split for validation dataset.\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Model used in notebook.\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# As Kaggle competition uses MEA as performance metric, we decided to use the same.\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "nVB_Fy3pG_zm"
      },
      "source": [
        "### Timestamp extraction\n",
        "\n",
        "Because crazy things happened in the past year,  we validated that, some specific dates had significantly higher electricity prices. Therefore we do weekday, month, and time extraction from the timestamp. \n",
        "\n",
        "We only extract timestap features.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "y3NmRhvzG_zm"
      },
      "outputs": [],
      "source": [
        "def extract_weekday(dataset):\n",
        "    splits = dataset['date'].astype(str).str.split('-')\n",
        "    dataset['weekday'] = [datetime.date(int(year), int(month), int(day)).weekday() for (year, month, day) in splits]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0orwSVNLG_zn"
      },
      "outputs": [],
      "source": [
        "def extract_month(dataset):\n",
        "    dataset['month'] = [month for (_, month, _) in dataset['date'].astype(str).str.split('-')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "DrOWkTuHG_zn"
      },
      "outputs": [],
      "source": [
        "def extract_datetime(dataset):\n",
        "    dataset.loc[:,'time'] = pd.to_datetime(dataset.loc[:,'time'], format=\"%Y-%m-%d %H:%M:%S\", utc=True)\n",
        "    dataset['date'] = dataset['time'].dt.date\n",
        "    dataset['hour'] = dataset['time'].dt.hour"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MJ63lOigG_zo"
      },
      "outputs": [],
      "source": [
        "def extract_features(dataset):\n",
        "    extract_datetime(dataset)\n",
        "    extract_month(dataset)\n",
        "    extract_weekday(dataset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### One Hot Encoder.\n",
        "\n",
        "As regressor models prefer numerical inputs and we copied this notebook from Nerual Networks, we decided to delete this method."
      ],
      "metadata": {
        "id": "D6sFltiFn6k2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "dfyY0rZaG_zn"
      },
      "outputs": [],
      "source": [
        "def one_hot_encode(dataset, columns, encoder = None) -> preprocessing.OneHotEncoder:\n",
        "    if encoder:\n",
        "        transformed = encoder.transform(dataset[columns])\n",
        "    else:\n",
        "        encoder = preprocessing.OneHotEncoder(sparse= False)\n",
        "        transformed = encoder.fit_transform(dataset[columns])\n",
        "\n",
        "    new_columns = []\n",
        "    for i, column in enumerate(encoder.feature_names_in_):\n",
        "        new_columns.extend([column + str(cat) for cat in encoder.categories_[i]])\n",
        "\n",
        "    encoder_df = pd.DataFrame(transformed, index=dataset.index)\n",
        "    dataset[new_columns] = encoder_df\n",
        "    dataset.drop(columns=columns, inplace=True)\n",
        "    return encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "hN7eX9_bG_zo"
      },
      "source": [
        "### Feature dropping\n",
        "\n",
        "In Estonia, there are approximately 500\\-800 millimeters of rain on average. Our dataset consisted of only about 140mm of rain, which is definitely not correct. Also, the amount of snow was inappropriate for the same reason. We could integrate a new wether dataset or leave it as it is. The simpliest approch is to delete the whole columns, which we decided to do.\n",
        "\n",
        "Some of the rows contained null values, which we also dropped. There was a case, when electricy prices hit market limit, which caused some outliners. It was wise to drop them.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wjBqWsJsG_zo"
      },
      "outputs": [],
      "source": [
        "def drop_features(dataset):\n",
        "    dataset.drop(columns=['snow','prcp','time','date'], inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0xEwUxN-G_zp"
      },
      "outputs": [],
      "source": [
        "def drop_rows(dataset):\n",
        "    # Deal with NaN values\n",
        "    initial_len = len(dataset)\n",
        "    dataset.dropna(inplace=True)\n",
        "    new_len = len(dataset)\n",
        "    if (initial_len != new_len):\n",
        "        print(f'Dropped {initial_len - new_len} row')\n",
        "\n",
        "    # Deal with outliners\n",
        "    dataset.drop(dataset[dataset['el_price'] > 1].index , inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalize\n",
        "\n",
        "Here we scale numeric values between 0 to 1 with MinMaxScaler"
      ],
      "metadata": {
        "id": "gT0au8IXp0H9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ywEAPeAXG_zp"
      },
      "outputs": [],
      "source": [
        "def normalize(dataset, scaler = None) -> (pd.DataFrame, preprocessing.MinMaxScaler):\n",
        "    if scaler:\n",
        "        dataset_scaled = scaler.transform(dataset)\n",
        "        return (dataset_scaled, scaler)\n",
        "    scaler = preprocessing.MinMaxScaler()\n",
        "    dataset_scaled = scaler.fit_transform(dataset)\n",
        "    return (dataset_scaled, scaler)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PCA\n",
        "\n",
        "As OneHotEncoder introduced a lot of new features, we decided to do PCA with leaving 90% of importance value."
      ],
      "metadata": {
        "id": "cOVXwwdup1eX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "w5mvJMC5G_zp"
      },
      "outputs": [],
      "source": [
        "def reduce_dimensions(dataset, pca = None) -> (pd.DataFrame, decomposition.PCA):\n",
        "    if pca:\n",
        "        dataset_reduced = pca.transform(dataset)\n",
        "        return (dataset_reduced, pca)\n",
        "    pca = decomposition.PCA(n_components=0.9)\n",
        "    dataset_reduced = pca.fit_transform(dataset)\n",
        "    return (dataset_reduced, pca)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocess\n",
        "\n",
        "Here we combine all the previous methods into one. \n",
        "As trained encoder must be used on test set, we return it from the method."
      ],
      "metadata": {
        "id": "e4GTbGrHrPZ8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "9ZgyuDOzG_zp"
      },
      "outputs": [],
      "source": [
        "def preprocess(dataset, encoder=None) -> preprocessing.OneHotEncoder:\n",
        "    extract_features(dataset)\n",
        "    drop_features(dataset)\n",
        "    encoder = one_hot_encode(dataset, ['coco', 'weekday'], encoder)\n",
        "    drop_rows(dataset)\n",
        "    return encoder\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "5VCuq2_ZG_zp"
      },
      "source": [
        "### Import dataset\n",
        "\n",
        "Here we import dataset, do inital processing and split into train and validation. As we predict consumption, we extract this column to separate numpy array."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "QrxVmltyG_zp"
      },
      "outputs": [],
      "source": [
        "def read_dataset(file_name) -> pd.DataFrame:\n",
        "    return pd.read_csv(file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "4M-OODkHG_zq"
      },
      "outputs": [],
      "source": [
        "def extract_labels(dataset) -> (pd.DataFrame, pd.Series):\n",
        "    X_train = dataset.loc[:, ~dataset.columns.isin(['consumption'])]\n",
        "    y_train = dataset['consumption']\n",
        "    return (X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86ZWVt3pG_zq",
        "outputId": "a7a1d51e-9c44-4dd3-bee8-7aa937572332"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dropped 2 row\n"
          ]
        }
      ],
      "source": [
        "train_df = read_dataset('train.csv')\n",
        "encoder = preprocess(train_df)\n",
        "\n",
        "\n",
        "X_train, y_train = extract_labels(train_df)\n",
        "\n",
        "X_train_norm, scaler = normalize(X_train)\n",
        "X_train_reduced, pca = reduce_dimensions(X_train_norm)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtaKcoPHG_zq",
        "outputId": "212c84f3-891a-4288-da10-b220db96a37a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8588, 41)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "X_train_norm.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJQrBk77G_zr",
        "outputId": "18b15950-7652-4359-93f9-e75da13adfb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(168, 41)\n"
          ]
        }
      ],
      "source": [
        "# Here we use all the previous encoders and scalers on test set.\n",
        "X_test = read_dataset('test.csv')\n",
        "preprocess(X_test, encoder)\n",
        "\n",
        "X_test_norm, _ = normalize(X_test, scaler)\n",
        "print(X_test_norm.shape)\n",
        "X_test_reduced, _ = reduce_dimensions(X_test_norm, pca)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ASksVPZVG_zr"
      },
      "outputs": [],
      "source": [
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_reduced, y_train, test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPrlZDSjG_zr",
        "outputId": "a7e8aa84-b341-457d-f9d3-e7e9949227e2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6870, 16)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RandomForestRegress\n",
        "\n",
        "We train a RandomForestRegressor model with train data and validate the output on validation set."
      ],
      "metadata": {
        "id": "7ia7UWWGsSRx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "-EZWjw3iG_zr",
        "outputId": "02243b9c-ab50-4ba3-8271-fcf7c4d3f61b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestRegressor()"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "rfr = RandomForestRegressor()\n",
        "rfr.fit(X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_val_predicted = rfr.predict(X_val)\n",
        "# We validate the model performance.\n",
        "mean_absolute_error(y_val_predicted, y_val)"
      ],
      "metadata": {
        "id": "dptGtBsYHYVv",
        "outputId": "0c23cdbe-734c-45b6-cf98-6ea483ea8dd8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5509063562281723"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prediction \n",
        "\n",
        "We predict on the test dataset and write the output to csv file."
      ],
      "metadata": {
        "id": "5D65NKv5taKh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = rfr.predict(X_test_reduced)\n",
        "display(prediction)"
      ],
      "metadata": {
        "id": "dNcUV4NELuj3",
        "outputId": "089dc183-04c8-4c83-bf60-c528be335357",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([0.3966 , 0.63415, 0.28624, 1.15627, 0.58738, 0.48478, 0.7549 ,\n",
              "       0.65639, 0.75048, 1.7137 , 0.80325, 0.82963, 0.93394, 0.64838,\n",
              "       0.62494, 0.65115, 1.08123, 1.01876, 0.95316, 0.94253, 0.87391,\n",
              "       1.24036, 0.71869, 0.63841, 0.77301, 0.65657, 0.66852, 0.70384,\n",
              "       0.5559 , 0.62269, 0.38614, 0.77853, 0.76771, 0.72248, 0.58045,\n",
              "       0.70981, 0.69656, 0.81392, 0.6975 , 0.86664, 0.73248, 0.79423,\n",
              "       0.78125, 0.70698, 0.85319, 0.69499, 0.59707, 1.22386, 0.99267,\n",
              "       0.37775, 0.58194, 0.62324, 0.90012, 0.8435 , 0.88093, 0.83026,\n",
              "       0.90277, 0.57675, 1.24543, 2.09502, 0.69632, 0.70843, 0.67123,\n",
              "       0.83993, 0.67182, 0.5946 , 0.74624, 0.65486, 0.56741, 0.58268,\n",
              "       0.72901, 0.67045, 0.74364, 0.46168, 0.3747 , 0.76511, 0.56798,\n",
              "       0.48638, 0.35104, 0.49472, 0.92808, 1.02863, 1.04069, 0.83119,\n",
              "       0.6467 , 0.75924, 0.75504, 1.37861, 0.9918 , 0.75365, 0.50903,\n",
              "       0.52133, 0.52613, 0.65696, 0.48495, 0.32465, 0.50681, 0.49564,\n",
              "       0.74685, 0.70117, 0.61931, 0.68171, 0.67306, 0.78943, 0.60355,\n",
              "       0.58943, 0.68451, 0.67244, 1.83339, 0.45662, 0.63798, 0.78369,\n",
              "       0.55694, 0.62379, 0.77973, 0.99159, 0.97759, 0.86349, 0.41232,\n",
              "       0.77186, 0.70244, 1.02182, 0.75562, 0.78003, 0.66545, 0.69361,\n",
              "       0.66357, 0.85197, 0.87703, 0.86779, 0.9131 , 0.77679, 0.84353,\n",
              "       0.68025, 0.694  , 0.66323, 0.77598, 0.59914, 1.11917, 0.77128,\n",
              "       0.78271, 0.79728, 1.11365, 0.96264, 0.7334 , 0.7841 , 0.8305 ,\n",
              "       0.67237, 0.64793, 0.90141, 0.86115, 0.61607, 0.59889, 0.5507 ,\n",
              "       0.55564, 0.66833, 0.59765, 0.58084, 0.58661, 0.68558, 1.00222,\n",
              "       1.48608, 1.6867 , 1.42984, 1.4555 , 0.74624, 1.6937 , 1.78648])"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We must reload the test dataframe, as we droped the time column.\n",
        "X_test = read_dataset('test.csv')\n",
        "\n",
        "predictions_dict = {'time':X_test.time,'consumption':prediction}\n",
        "pred_df = pd.DataFrame(predictions_dict)\n",
        "pred_df.to_csv('submission_RFR_V1.csv',index=False)"
      ],
      "metadata": {
        "id": "XKzfaUpNJtQE"
      },
      "execution_count": 22,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (system-wide)",
      "language": "python",
      "metadata": {
        "cocalc": {
          "description": "Python 3 programming language",
          "priority": 100,
          "url": "https://www.python.org/"
        }
      },
      "name": "python3",
      "resource_dir": "/ext/jupyter/kernels/python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}